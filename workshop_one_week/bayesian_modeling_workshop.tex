
 \pdfoutput=1

\documentclass{article} % For LaTeX2e

%STANDARD PREAMBLE
%https://tex.stackexchange.com/questions/68821/is-it-possible-to-create-a-latex-preamble-header
\usepackage{/Users/mwojno01/Research/Learning/latex_preamble/preamble}

\begin{document}

\title{Introduction to Bayesian Modeling} 

\maketitle
\tableofcontents

\section{Overview}

\subsection{Goal}  The goal of this workshop is to introduce students to the concepts and practice of Bayesian modeling.  

\subsection{Target Audience}  We expect that the typical student will be a graduate student, faculty member, staff member, or researcher in a quantitative field (such as computer science,  statistics,  engineering, or biology),  who would like to learn more about Bayesian modeling.  

\subsection{Prerequisites} Prerequisites include calculus,  some linear algebra, and some familiarity with introductory probability (e.g.,  we will assume prior familiarity with concepts such as expectation,  conditional probability,  and commonly used distributions,  such as Gaussian and Poisson).    The class will use Python as a common language.    The workshop will employ student-centered components so \textbf{be prepared to spend 3-4 hours a day outside of the workshop working on material.}  This time will be spent on  background reading and preparing code/demos/materials for the interactive component of the workshop.

\subsection{Textbook}

We will use \cite{gelman2013bayesian},  available online at \url{http://www.stat.columbia.edu/~gelman/book/}. 

\subsection{Philosophy}

\textit{Learning in order to create} is both more fun and more effective than \textit{learning for some extrinsic purpose}.   Hence,  the workshop is structured so as to (a) be student-centered and (b) allow self-determination and autonomy in how students engage with the material. 

\subsection{Format}

About half of the workshop will involve lectures via slides.   

About half of the workshop will be interactive,  including:

\begin{itemize}
\item Student ``lightning chat" (10 minute) presentations.   Something like one per student per workshop,  depending on the number of students.    To allow for student-centered direction and autonomy,  students may choose any of the following:
	\begin{itemize}
	\item Presentation of Python implementations of models from \cite{hoff2009first} ,  \cite{ gelman2013bayesian},  or the workshop.
	\item Presentation of an exercise from \cite{gelman2013bayesian} or \cite{hoff2009first}.
	\item Reviewing a demo with us from \url{https://github.com/avehtari/BDA_py_demos}.
	\item Presentation of a reading section,  blog,  etc.  of interest.
	\item Presentation of a mathematical derivation of something relevant to the course.
	\item Presentation of how a concept relates to something from their research area.
	%\item A list of questions that we can discuss.
	\end{itemize}

\item Real-time python applications lab -- Google Collab exercises ?  Python (rather than R) implementations of \cite{hoff2009first} and \cite{gelman2013bayesian}.
\item Mini reading group discussions  -- They might not have time to read a whole paper,  but we could discuss sections of a text at the very beginning, or perhaps sections of a relevant paper.
\end{itemize}


\section{Topics}

Below are topics we plan to cover in the course:

\subsection{Introduction to Bayes}

We present everything in here using conjugate models with closed-form posteriors.  The models are useful in and of themselves,  as well as to build intuition for more complicated models.   

Primary references here are \cite{hoff2009first} and \cite{gelman2013bayesian}.

\begin{itemize}
\item \textbf{Why Bayes?} -- See Section 1.2 of \cite{hoff2009first}.   \cite{bishop2006pattern} has some nice plots motivating why use Bayesian linear regression over standard linear regression.   \cite{ghahramani2013bayesian}  has some nice plots illustrating the Bayesian approach and how it mitigates overfitting.   I can provide a nice example with biometric profiling of human typing dynamics.   \cite{held2006bayesian} has a nice simple example of obtaining non-standard functionals from the posterior that can be of interest.   \cite{wilson2020case} presents the case for Bayesian deep learning.  
\item \textbf{Belief functions,  Bayes rule} -- Sections 2.1,  2.2 of \cite{hoff2009first}.   \cite{ghahramani2013bayesian} briefly overviews of the Bayesian framework.  \textit{Why most published research findings are false} \cite{ioannidis2005most} provides nice motivation.    Could perhaps cover exchangeability here. 
\item \textbf{Binomial, Poisson, normal, multivariate normal models} -- Sections 3.1,  3.2, 5, and 5 of \cite{hoff2009first}.     Introduce the exponential family formalism \cite{wojnowiczXXXXexponential} for much greater breadth.   
\item \textbf{Bayesian linear regression} -- Section 9 of \cite{hoff2009first}.     I have notes on this.   There are some nice slides here which also illustrate the use of kernels.\footnote{Nice Bayesian linear regression slides: \url{https://www.cs.toronto.edu/~rgrosse/courses/csc411_f18/slides/lec19-slides.pdf}}   Introduce model selection here (Section 9.3 of \cite{hoff2009first}. 
\item \textbf{Bayesian workflow} -- Lots of nice resources for Bayesian workflow.    For example: \cite{gelman2020bayesian} or \cite{gabry2019visualization}.   Section 6 of \cite{gelman2013bayesian} covers model checking.       Some points to make re: model checking
	\begin{itemize}
	\item \textit{Samples from the posterior predictive should capture important properties of the observed dataset.}  For a violation of this,  see the normal model for Newcomb's speed of light measurements.  (Compare Figures 6.2 and 3.1 of \cite{gelman2013bayesian}.)
	\end{itemize}
\end{itemize}

We will want to find a way to get students to group up,  probably based on domain expertise/interests,  so that they can eventually work together on a project.  

\subsection{Methods}

We introduce these methods,  which can be used for models without closed-form posteriors.  We practice applying them in the next section.  

\begin{itemize}
\item \textbf{MCMC} - \blue{Karin} will present,  including an introduction to pymc3. 
\item \textbf{Variational inference} \cite{wojnowiczXXXXfoundations}.
\end{itemize} 

\subsection{More complicated models} \label{sec:more_complicated_models}

Here are some models which are still fairly standard,  but lack conjugate priors,  and so inference typically requires VI or MCMC.     \blue{Karin: Where here,  or elsewhere,  would you like to illustrate applications of MCMC?}

\begin{itemize}
\item \textbf{Hierarchical models}  Hierarchical normal model (e.g. Gelman's 5 schools example).   Hierarchical linear regression (Chapter 13 of \cite{gelman2013bayesian},   Secs 11.1-11.3 of \cite{hoff2009first}.).  Figure 11.1 and 11.3 (right) of \cite{hoff2009first} nicely shows the beneficial effect of sharing statistical strength in a hierarchical linear regression,  as compared to many separate linear regressions.
\item \textbf{Regression models for binary and multi-class data} Includes logistic regression, probit regression,  binomial,  multinomial,  etc.    Use this to cover additional inference techniques:  auxilliary variable trick and Laplace variational inference.    Show demo ``beating scikit-learn with variational inference."   See also pp.  390 of \cite{hoff2009first} for a useful warm-starting strategy.   Could generalize to Bayesian GLMs.   Could also cover or mention hierarchical extensions (i.e.  Bayesian GLMM's).   
\item \textbf{Mixture models} I will give CAVI for Gaussian mixture models. 
\item \textbf{Time series models}  Probably just hidden markov models,  although would be nice to also introduce state space models.    Could mention embedding of GLM's or GLMM's within them.   May give some overview to probabilistic graphical models here.  
\item \textbf{Bayesian Deep Learning}  Bayes and neural networks.   20-30 min w/ guest presenter,  Kyle Heuton,  Ph.D.  student,  computer science.
\end{itemize}

%\subsection{Even more complicated models} 
%
%Here we discuss models which have additional complexity -- they could involve neural networks,  non-parametrics,  larger scale,  etc.    Often these involve some additional inferential machinery -- \textit{stochastic} variational inference,  reparametrization trick,  etc.
%
%%The likelihood that students would use this material for their projects is relatively low,  so this would make a nice set of things to cover as we begin transitioning over to the student project day.
%
%\begin{itemize}
%\item \textbf{Why Bayesian Deep Learning?}  Bayes and neural networks.   20-30 min w/ guest presenter,  Kyle Heuton,  Ph.D.  student,  computer science.
%\item \textbf{Custom models}  Could present black-box variational inference or automatic differentiation variational inference here.   I have some notes that I could convert to slides.  Would take some work though.  \blue{Anna,  perhaps you'd be interested,  since you've been working with this?}
%\item \textbf{Sampler platter of other possible topics}   VAE,   Gaussian processes,  composing time series models with neural networks,  indian buffet processes,  dirichlet process mixture models,  etc.  
%\end{itemize}

%\subsection{Bayesian workflow}  \label{sec:Bayesian workflow}
%
%
%Lots of nice resources for Bayesian workflow.    For example: \cite{gelman2020bayesian} or \cite{gabry2019visualization}.   Section 6 of \cite{gelman2013bayesian} covers model checking.       
%
%We could cover this the day before the student projects -- which should help them as they wrap up their projects -- and leave lots of time/space for workshopping. 
%
%Some points to make re: model checking
%
%\begin{itemize}
%\item \textit{Samples from the posterior predictive should capture important properties of the observed dataset.}  For a violation of this,  see the normal model for Newcomb's speed of light measurements.  (Compare Figures 6.2 and 3.1 of \cite{gelman2013bayesian}.)
%\end{itemize}
%
%\subsection{Student project presentations}
%
%For student projects,  we could have students present results of a Bayesian data analysis mini-project.   Perhaps they could highlight some aspect of the Bayesian workflow along the way. 
%
%We would probably want to have a ``workshopping" section the day before they do their presentations.


\bibliography{references_bayesian_modeling_workshop}{}
\bibliographystyle{unsrt}


\appendix

\section{Resources which may be appropriate for mini-reading group or student presentations}

\begin{itemize}
\item Intro: \textit{Why most published research findings are false. } \cite{ioannidis2005most}
\item Model checking: Bayesian workflow.
\item Textbook sections TBD. 
\end{itemize}

\section{Additional problems to work on}

\subsection{Batting average dataset}

The hierarchical normal model for (arcsine-transformed) batting average data on pp.  163 of \cite{gelman2013bayesian} has some serious deficiencies,  as exposed in Table 6.1 in the section on model checking.    

Can you construct (and learn) a better model which makes predictions closer to the true final batting average?
 
Examples:
\begin{itemize}
\item Add an extra layer to the hierarchy,  so that player $p$'s 1970 batting average inherits from player $p$'s overall batting average which in turn inherits from a population batting average.  (Of course,  I am speaking of the arcsine-transformed batting averages,  so that we can use a hierarchical normal model.)
\item Add an autoregressive component, because,  as mentioned by Gelman,  player batting averages \textit{DO} change over time. 
\end{itemize}

The text also does a poor job of checking the modeling assumption violations that were of concern.   Can you do a better job of checking them,  and if necessary,  address them?

Examples:
\begin{itemize}
\item If batting averages are indeed heavy tailed or skewed,  move from a normal distribution to something else.   For example,  could try a t-distribution with Laplace inference to handle the non-conjugacy. 
\item If the variance is indeed too high for a binomial model,  try something that can handle the overdispersion. 
\end{itemize}

\section{More detail on slides}

\subsection{Why Bayes}

\paragraph{Estimating the probability of a rare event}
See Section 1.2.1 of \cite{hoff2009first}.  The problem is to estimate the proportion of people in a small town that have a disease given a small sample of 20 individuals.     This is a nice believable example in which prior information is natural: we use prevalence in similar towns.  (Make a note that really this is foreshadowing hierarchical models and hierarchical regressions!)  This also illustrates another nice property of Bayes  -- we can get lots of functionals from the posterior.  The plot on the right of Figure 1.1 -- showing the shift between the prior and posterior distributon -- is a classic.   The sensitivity analysis is pretty cool -- see the right hand plot of Figure 1.2,  as well as the last couple sentences on pp.7.    

The REAL kicker,  I think,  is the comparison to non-Bayesian methods.  The frequentist confidence interval is complete garbage here,  and the ``adjusted" Wald interval is clearly just a (very specific) choice of .prior.   Nice opportunity for discussion here.  Ask:  what's the advantage of Bayes over that?  Possible answers: That is seemingly ad hoc; it's not felxible to other choice of priors; Bayes makes it clear how it relates to priors; and it doesn't allow for sensitivity analysis (or investigation of various functionals).  


%\section{Need to do}
%
%\begin{itemize}
%\item \blue{Karin}  Bring pymc3,  stan,  etc.  into this -- will make things a lot more useful for the audience than requiring that they code things up from scratch!
%\end{itemize}

%\subsection{Regression models for binary and multi-class data}
%
%Take the batting average problem on pp.  163 of \cite{gelman2013bayesian} and use a more conventional modeling approach (logit or probit regression) than what was described in the text (using the arcsine transformation).   What kinds of inference results do you get?  How far off is  Polya-Gamma variational inference?  What about Laplace variational inference? 


          


\end{document}
