\documentclass[10pt]{beamer}

%STANDARD PREAMBLE
%https://tex.stackexchange.com/questions/68821/is-it-possible-to-create-a-latex-preamble-header
\usepackage{/Users/mwojno01/Research/Learning/latex_preamble/beamer_preamble}

%
%% ALLOW FOR ITEMIZE ENVIRONMENTS WITH NO PRECEDING
% SPACING, IF DESIRED
% Reference: https://tex.stackexchange.com/questions/86054/how-to-remove-the-whitespace-before-itemize-enumerate
%\usepackage{enumitem}% http://ctan.org/pkg/enumitem 
\usepackage{paralist}

% RANDOM VARIABLE
\newcommand{\x}{X}
\newcommand{\y}{Y}


\title{Bayesian Inference: Intro via Conjugacy}

\begin{document}

\maketitle






\begin{frame}{Bayes' Rule}
\footnotesize

\begin{sblock}{Bayes' Rule}
\[ \underset{posterior}{p(\theta|x)} = \df{ \underset{likelihood}{p(x|\theta)} \quad  \underset{prior}{p(\theta)} }{ \underset{evidence}{p(x)}}  = \dfrac{p(x | \theta) p(\theta)}{\int p(x | \theta) p(\theta) \, d\theta}  \]
\end{sblock}

\begin{sblock}{Posterior}
The posterior distribution is proportional to the prior times the likelihood:
\[p(\theta | x) \propto p(x | \theta) p(\theta)\]

The posterior distribution \textit{is a distribution} over $\theta$.

\end{sblock}

\begin{sblock}{Evidence}
The evidence, or \textit{marginal likelihood}, can be used for model comparison.
\end{sblock}

\end{frame}


%%%%%%%%%

\begin{frame}{Bayesian inference: conjugate example}
Sometimes, we can compute the posterior distribution by hand, given prior and likelihood.

\begin{sblock}{Setup: flipping a coin}
Probability that it lands heads is (unknown) $\theta$. \\
Prior probability over $\theta$ assumed to follow a $Beta(3,3)$ distribution:
$$ p(\theta) = \frac{\theta^{3-1}(1-\theta)^{3-1}}{B(3,3)}$$
Note: $\theta \sim Beta(a, b)$ means $p(\theta) \propto \theta^{a-1}(1-\theta)^{b-1}$\\
Will collect data by flipping coin once. Likelihood of observing heads ($x=1$) or tails ($x=0$) is given by a Bernoulli distribution:
$$p(x | \theta) = \theta^x(1-\theta)^{1-x} $$.
\end{sblock}


\end{frame}

\begin{frame}{Bayesian inference: conjugate example}

\begin{sblock}{Setup: flipping a coin}
Probability that it lands heads is (unknown) $\theta$. \\
Prior probability over $\theta$ assumed to follow a $Beta(3,3)$ distribution:
$$ p(\theta) = \frac{\theta^{3-1}(1-\theta)^{3-1}}{B(3,3)}$$
Note: $\theta \sim Beta(a, b)$ means $p(\theta) \propto \theta^{a-1}(1-\theta)^{b-1}$\\
Will collect data by flipping coin once. Likelihood of observing heads ($x=1$) or tails ($x=0$) is given by a Bernoulli distribution:
$$p(x | \theta) = \theta^x(1-\theta)^{1-x} $$.
\end{sblock}
\begin{sblock}{Computing the posterior after observing x=1}
$$p(\theta| x) \propto p(x | \theta) p(\theta) = \theta^1(1-\theta)^{0}  \theta^{2}(1-\theta)^{2}  = \theta^3 (1-\theta)^2 \implies \theta | x \sim Beta(4,3)$$
\end{sblock}

\end{frame}

%%%%%%%%%%%%%%%

\begin{frame}{Bayesian inference: conjugacy}

\begin{sblock}{The idea}
We have conjugacy when the prior and the posterior distributions are in the same family (e.g. in the previous example, the prior and posterior are beta distributions).
\end{sblock}

\begin{sblock}{Definition}
Conjugacy can be defined as follows \cite{gelman2013bayesian}. If $\mathcal{F}$ is a class of sampling distributions and $\mathcal{P}$ is a class of prior distributions for $\theta$, then the class $\mathcal{P}$ is \textit{conjugate} for $\mathcal{F}$ if
\[  p(\theta \cond y ) \in \mathcal{P} \; \text{for all} \; p(\cdot \cond \theta) \in \mathcal{F} \; \text{and} \; p(\cdot) \in \mathcal{P} \]
\end{sblock}

\bottomtext{Technical note: the condition trivially holds if $\mathcal{P}$ is taken to be the space of all probability distributions!}
\end{frame}


%%%%%%%%%%%%%%

\begin{frame}{Bayesian inference: Posterior predictive distribution}

\begin{sblock}{Given}

$p(\theta | x)$ - posterior \\
$p(\theta)$ - prior \\
$p(x | \theta)$ - likelihood

\end{sblock}

\begin{sblock}{Posterior predictive distribution}

Consider the probability of new data $x'$. Posterior predictive distribution is:

$p(x' | x) = \int p(x', \theta | x) \, d\theta = \int p(x' | \theta, x) p(\theta |x) \, d\theta = \int p(x' | \theta) p(\theta | x) \, d\theta$

Incorporates the knowledge and uncertainty about $\theta$ that we still had after seeing data $x$.

\end{sblock}

\end{frame}

%%%%%%%%%%%%%%%

\begin{frame}{Bayesian inference: tractability notes}

Generally, computing the posterior distribution is much harder than in this example!
\vfill
Consider the denominator in $ p(\theta | x) = \dfrac{p(x | \theta) p(\theta)}{\int p(x | \theta) p(\theta)} \, d\theta $  - integrals are hard 
\vfill
In nonconjugate examples, we need approaches to work with the posterior distribution when we cannot calculate it directly. Stay tuned!

\end{frame}



%%%%%%%%%




%%%%%%%%%%%%%%%%







\end{document}