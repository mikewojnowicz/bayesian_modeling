
%% Refer to latex document when possible
\pdfoutput=1
\documentclass[10pt]{beamer}

%STANDARD PREAMBLE
%https://tex.stackexchange.com/questions/68821/is-it-possible-to-create-a-latex-preamble-header
\usepackage{/Users/mwojno01/Research/LabMeetings/beamer_preamble}




\title{The Multivariate Normal Model}
%\subtitle{}

\begin{document}

\maketitle

\begin{frame}{Table of contents}
  \setbeamertemplate{section in toc}[sections numbered] \tableofcontents[hideallsubsections]
\end{frame}


\section{Overview}

\begin{frame}{Some motivations for the normal}

\begin{figure}
\includegraphics[width=.3\textwidth]{images/mvn}
\end{figure}

\begin{itemize}
	\item \textit{Maximum entropy} among all distributions with a given mean $\+\mu$ and variance $\+\Sigma$.  \pause
	\item Characterized by independence of sample mean and sample variance. {\footnotesize (Bayesian take: ask if your beliefs about the sample mean are independent from those about the sample variance.)} \pause
	\item Sample averages are generally approximately normally distributed due to the Central Limit Theorem. \pause 
	\item Sufficient statistics are sample mean and variance; so will consistently estimate population mean and variance even for non-normal distributions.
\end{itemize}
\end{frame}


\begin{frame}{Why \alert{Bayesian} normal? }
\begin{itemize}
\item Prior information often exists and can be taken into account.
	\begin{itemize}
	\item Population-level info: see the typing example, PIMA Indians example
	\item Nature (e.g. support) of data: see the reading comprehension example 
	\end{itemize} \pause 	
\item ML estimates of covariance matrices have large variance.
	\begin{itemize}  
	\item Problem can be especially bad in certain contexts (e.g., small data, high-dimensions, missing data)
	\item Spherical prior provides regularization	\item Posterior asymptotically concentrates around maximum likelihood (ML) solution
	\end{itemize} \pause 
\item Inference is no harder than for frequentist models
	\begin{itemize}
	\item Easy, cheap updates {\scriptsize (a conjugate prior exists)}
	\item Supports online learning 	
	\item Fits nicely in more complex models	
	\item Nice hyperparameter interpretation
	\end{itemize}
\end{itemize}	
\end{frame}

\section{Conjugate inference}

\begin{frame}{A conjugate prior}
\red{TODO: Fill in}
\end{frame}

\begin{frame}{Application: Modeling typing dynamics}

See powerpoint slides.	
\end{frame}

%%%%%
\section{Semi-conjugate inference}

\begin{frame}{Semi-conjugate Bayesian normal}

\begin{sblock}{Semi-conjugate Bayesian MVN}
Consider the following model with a normal sampling distribution and \textit{semi-conjugate} prior 
\begin{align*}
\+\mu &\sim \N_{d}(\+m_0,\+V_0 ) \\
\+\Sigma &\sim \InverseWishart(\nu_0,  \+\Psi_0) \\
\+x_i \cond \+\mu,  \+\Sigma &\iid \N_{d}( \+\mu , \+\Sigma), \quad i=1,...,N
\end{align*}
We define $\+x := (\+x_1,  \hdots \+x_N)$,  where each $\+x_i \in \R^d$.
\end{sblock}
\pause 

\vfill
\begin{sblock}{Fully conjugate vs semi-conjugate MVNs}
This model is different than the model with the fully conjugate (Normal-Inverse-Wishart) prior on the pair $(\+\mu, \+\Sigma)$.   The conditionally conjugate prior lacks closed-form posterior updating,  but is also more expressive.    It is also easier to extend upwards.
\end{sblock}

\end{frame}

\begin{frame}{Semi-conjugate models generally}

\begin{sblock}{Conjugate models}
Conjugacy can be defined as follows \cite{gelman2013bayesian}. If $\mathcal{F}$ is a class of sampling distributions and $\mathcal{P}$ is a class of prior distributions for $\theta$, then the class $\mathcal{P}$ is \textit{conjugate} for $\mathcal{F}$ if
\[  p(\theta \cond y ) \in \mathcal{P} \; \text{for all} \; p(\cdot \cond \theta) \in \mathcal{F} \; \text{and} \; p(\cdot) \in \mathcal{P} \]
\end{sblock}

\begin{sblock}{Semi-conjugate models}
Conditional conjugacy (sometimes called semi-conjugacy) can be defined similarly  \cite{gelman2013bayesian}.  If $\mathcal{F}$ is a class of sampling distributions and $\mathcal{P}$ is a class of prior distributions for $\theta \cond \phi$, then the class $\mathcal{P}$ is \textit{conditionally conjugate} for $\mathcal{F}$ if
\[  p(\theta \cond \phi, y) \in \mathcal{P} \; \text{for all} \; p(\cdot \cond \theta, \phi) \in \mathcal{F} \; \text{and} \; p(\cdot \cond \phi) \in \mathcal{P} \]
\end{sblock}
\pause 
\bottomtext{In other words,  a family of prior distributions for a parameter is called conditionally conjugate if the conditional posterior distribution (often called the \textit{complete conditional}),  given the data and all other parameters in the model,  is also in that class.}
\end{frame}

\begin{frame}{We {\red \heart} semi-conjugacy}	 
 Why are conditionally conjugate models of interest?  The posterior distributions for conditionally conjugate models are easily approximated with Gibbs sampling or Mean Field Variational Inference -- the former samples from the complete conditional,  whereas the latter takes variational expectations with respect to the natural parameter of the complete conditional.   
 
\end{frame}


	
\begin{frame}{Complete conditionals for the Bayesian MVN}
	
We sample from the posterior by iteratively sampling from the \textit{complete conditionals}:
\footnotesize 
\begin{align*}
\+\mu  \cond \+\Sigma, \+x &\sim \N_{d}(\+m,\+V ) \\ %\labelit\label{eqn:normal_model_complete_conditional_on_mu} \\
\intertext{where}
\+m  &=  \bp{\+V_0^{-1} + N  \+\Sigma^{-1} }^{-1}  \bp{\+V_0^{-1} \+m_0 + N \+\Sigma^{-1}  \bar{\+x} } \\
\+V &= \bp{\+V_0^{-1} +  N \+\Sigma^{-1} }^{-1} \\
\intertext{and}
\+\Sigma \cond \+\mu,  \+x  &\sim \InverseWishart(\nu,  \+\Psi) \\
%\labelit \label{eqn:normal_model_complete_conditional_on_Sigma} \\
\intertext{where}
\nu &=  \nu_0 + N \\
\+\Psi &= \Psi_0 + \ds\sum_{i=1}^N  (\+x_i - \+\mu) (\+x_i - \+\mu)^T 
%\labelit \label{eqn:Sigma_cc_normal_model_cond_conj_prior}\\ 
\end{align*}
\end{frame}


 
\begin{frame}{Complete conditionals: Interpretation}
These complete conditionals have nice interpretations:
\begin{itemize}
\item \textbf{Complete conditional for $(\+\mu  \cond \+\Sigma, \+x)$}: On the precision scale,  $\+V$ is the sum of the prior precision matrix $\+V_0^{-1}$ and $N$ copies of the precision for each observation,  $\+\Sigma^{-1}$.    Similarly,  $\+m$ is the precision-weighted convex combination of $\+m_0$, the prior mean,    and the empirical average, $\bar{\+x}$.
\item \textbf{Complete conditional for $(\+\Sigma \cond \+\mu,  \+x)$}:  The covariance was estimated from $\nu$ observations with a sum of pairwise deviation products $\Psi$.
\end{itemize}
\end{frame}

\begin{frame}{Complete conditionals: Proof}

See exponential family notes. 

\end{frame}
	
	

%%%%%
\section{Application: Reading Comprehension}

\begin{frame}

See ipython notebook. 	
\end{frame}


%%%%%
\section{Missing data and imputation}

\begin{frame}{References}

\begin{center}
\includegraphics[width=.5\textwidth]{images/hoff_book}
\end{center}

\end{frame}

\begin{frame}{Pima Dataset}

\begin{figure}
\includegraphics[width=.6\textwidth]{images/pima_scatterplots}
\caption{  \footnotesize Univariate histograms and bivariate scatterplots for four variables taken from a dataset involving health-related measurements on 200 women of Pima Indian heritage living near Phoenix,  Arizona.  \scriptsize The four variables are \texttt{glu} (blood plasma glucose concentration),  \texttt{bp} (diastolic blood pressure),  \texttt{skin} (skin fold thickness),  and  \texttt{bmi} (body mass index).  }
\end{figure}

\end{frame}

\begin{frame}{Pima Dataset}

\begin{figure}
\includegraphics[width=.6\textwidth]{images/pima_first_ten_entries}
\caption{  \footnotesize Entries for the first ten subjects in the dataset.  \scriptsize The \texttt{NA}'s stand for ``not available." }
\end{figure}
\end{frame}

\begin{frame}{Description of problem}
How to do parameter estimation in the presence of missing data? \\
\begin{itemize}
\item[]  \footnotesize  We cannot do parameter estimation,  because we cannot compute the likelihood $\prod_{i=1}^n p(\+y_i \cond \+\theta)$.  \normalsize
\end{itemize}
\vfill \vfill 
Two common approaches taken by software packages:
\begin{enumerate}
\item Throw away all subjects with missing data
\pause  \\
\quad \redx \footnotesize Discards a potentially large amount of useful information.  \normalsize
\pause \\
\item  Impute the population mean or some other fixed value. 
\pause \\
\quad \redx \footnotesize  Assumes certainty about these values,  when in fact we have not observed them.  \normalsize
% DELIVERY: Elicit audience suggestions as to the problems with each common approach.
\end{enumerate}
\end{frame}


\begin{frame}{Missing at random (MAR)}
Let $\+O_i = (O_1, ..., O_p)^T$ be a binary vector such that
\begin{itemize}
\item $O_{ij} = 1 \implies  Y_{ij}  \; \text{is observed}$ 
\item $O_{ij} = 0 \implies  Y_{ij}  \; \text{is missing}$ 
\end{itemize}

\vfill

\begin{definition}
We say the missing data are \textit{missing at random} if $\+O_i$ and $\+Y_i$ are conditionally independent given the model parameters  $\+\theta$  and the distribution of $\+O_i$ does not depend on $\+\theta$.
% TODO: Check this definition;  I altered Hoff's to be more clear based on what seemed to be used in the upcoming formula.
\end{definition}


\vfill  \vfill \vfill  \pause 

\scriptsize
\textbf{Remark.}  This is one of the three types of missingness.   In gist:
\begin{itemize}
\item Missing completely at random (MCAR) - missingness is independent of all data
\item Missing at random (MAR) - missingness is independent of observed data
\item Missing not at random (MNAR) - missingness depends on missing values (and perhaps observed data)
\end{itemize}
% QUESTION: Does this characterization (grabbed from some slides) comport with the one I gave for MAR above? No mention of parameters here.   Perhaps the frequentist characterization differs form the Bayesian one?


\end{frame}


\begin{frame}{The likelihood in the presence of MAR data}

When the data is missing at random,  the sampling probability (density) for the data from observational unit $i$ is given by

\begin{align*}
p(\+o_i,  \set{y_{ij} : o_{ij} =1} \cond \+\theta) & \stackrel{(1)}{=} p(\+o_i) \;  p(\set{y_{ij} : o_{ij} =1} \cond \+\theta) \\
&=  p(\+o_i) \;  \ds\int p(y_{i1}, ...., y_{ip}  \cond \+\theta)  \prod_{y_{ij} : o_{ij} = 0} dy_{ij}
\end{align*}
\footnotesize where in (1) we applied the definition of MAR. \normalsize

\vfill \vfill \pause

\greencheck So in the presence of MAR data, the correct thing to do is \textit{integrate over} the missing data to obtain the marginal probability (density) of the observed data. 
\end{frame}


\begin{frame}{Utilization in multivariate normal models}


In the case of multivariate normal models \footnotesize (so $\+\theta = (\+\mu, \+\Sigma)$) \normalsize, the integration is easy:  Multivariate normals have normal marginals. 

\vfill \vfill \vfill 
\begin{example}
Suppose $\+y_i = (y_{i1},  \texttt{NA},  y_{i3},  \texttt{NA})^T$, so $\+o_i = (1,0,1,0)^T$.     

Then

\begin{align*}
p(\+o_i,  y_{i1},  y_{i3} \cond \+\mu, \+\Sigma) & = p(\+o_i) \;  p(y_{i1},  y_{i3} \cond \+\mu, \+\Sigma) \\
&=  p(\+o_i) \;  \ds\int p( \+y_i  \cond \+\mu, \+\Sigma)  \; dy_2 \; dy_4
\end{align*}
\vfill 

The marginal density $p(y_{i1},  y_{i3} \cond \+\theta)$ is simply a bivariate normal density with mean $(\mu_1,  \mu_3)^T$ and covariance matrix made up of $(\sigma^2_1,  \sigma_{13},  \sigma^2_3)$.
\end{example}

\end{frame}

\begin{frame}{\red{TODO}}

\begin{itemize}
\item Inference: Show how to adjust Gibbs sampling in the presence of missing data (see Hoff pp. 117-pp.118;  also make sure the notation,  and the use of semiconjugate vs conjugate prior,  aligns with how I set up the multivariate normal initially -- earlier on in this section of the course)
\item Correlations - discuss how to construct the posterior correlation matrix from the Gibbs samples (and note this is another example of the Bayesian paradigm yielding unlimited access to posterior functionals of interest,  without doing any extra inferential work).    Show the specific values on pp.119,  and the left hand side of Fig 7.4.   This is good to show because it is something you'd probably want out of a normal model anyways,  and also becuase it is needed for the next point.
\item Show true values vs posterior expectations of the missing data (Hoff Fig 7.5)   Mention that we get better predictions for skin and bmi,  due to their higher correlations.  Highlight how much better the posterior expectation is than a flat fixed value.
\end{itemize}

\end{frame}



\end{document}



