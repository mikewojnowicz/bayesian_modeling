
%% Refer to latex document when possible
\pdfoutput=1
\documentclass[10pt]{beamer}

%STANDARD PREAMBLE
%https://tex.stackexchange.com/questions/68821/is-it-possible-to-create-a-latex-preamble-header
\usepackage{/Users/mwojno01/Research/LabMeetings/beamer_preamble}




\title{The Multivariate Normal Model}
%\subtitle{}

\begin{document}

\maketitle

\begin{frame}{Table of contents}
  \setbeamertemplate{section in toc}[sections numbered] \tableofcontents[hideallsubsections]
\end{frame}


\section{Overview}

\begin{frame}{Some motivations for the normal}

\begin{figure}
\includegraphics[width=.3\textwidth]{images/mvn}
\end{figure}

\begin{itemize}
	\item \textit{Maximum entropy} among all distributions with a given mean $\+\mu$ and variance $\+\Sigma$.  \pause
	\item Characterized by independence of sample mean and sample variance. {\footnotesize (Bayesian take: ask if your beliefs about the sample mean are independent from those about the sample variance.)} \pause
	\item Sample averages are generally approximately normally distributed due to the Central Limit Theorem. \pause 
	\item Sufficient statistics are sample mean and variance; so will consistently estimate population mean and variance even for non-normal distributions.
\end{itemize}
\end{frame}


\begin{frame}{Why \alert{Bayesian} normal? }
\begin{itemize}
\item Prior information often exists and can be taken into account.
	\begin{itemize}
	\item Population-level info {\tiny (Previous example: disease prevalence.  Forthcoming example: biometrics and PIMA Indians)}
	\item Nature (e.g. support) of data {\tiny (Forthcoming example: reading comprehension)}
	\end{itemize} \pause 	
\item ML estimates of covariance matrices can have large variance.
	\begin{itemize}  
	\item Problem can be especially bad in certain contexts {\tiny ( e.g., small data, high-dimensions, missing data)}
	\item Spherical prior provides regularization	\item Posterior still asymptotically concentrates around maximum likelihood (ML) solution
	\end{itemize} \pause 
\item Inference is no harder than for frequentist models
	\begin{itemize}
	\item Easy, cheap updates {\scriptsize (a conjugate prior exists)}
	\item Supports online learning 	
	\item Fits nicely in more complex models	
	\item Nice hyperparameter interpretation
	\end{itemize}
\end{itemize}	
\end{frame}

\section{Conjugate inference}

\begin{frame}{A conjugate prior}
\red{TODO: Fill in}
\end{frame}

\begin{frame}{Application: Modeling typing dynamics}

See powerpoint slides.	
\end{frame}

%%%%%
\section{Semi-conjugate inference}

\begin{frame}{Semi-conjugate Bayesian normal}

\begin{sblock}{Semi-conjugate Bayesian MVN}
Consider the following model with a normal sampling distribution and \textit{semi-conjugate} prior 
\begin{align*}
\+\mu &\sim \N_{d}(\+m_0,\+V_0 ) \\
\+\Sigma &\sim \InverseWishart(\nu_0,  \+\Psi_0) \\
\+x_i \cond \+\mu,  \+\Sigma &\iid \N_{d}( \+\mu , \+\Sigma), \quad i=1,...,N
\end{align*}
We define $\+x := (\+x_1,  \hdots \+x_N)$,  where each $\+x_i \in \R^d$.
\end{sblock}
\pause 

\vfill
\begin{sblock}{Fully conjugate vs semi-conjugate MVNs}
This model is different than the model with the fully conjugate (Normal-Inverse-Wishart) prior on the pair $(\+\mu, \+\Sigma)$.   The conditionally conjugate prior lacks closed-form posterior updating,  but is also more expressive.    It is also easier to extend upwards.
\end{sblock}

\end{frame}

\begin{frame}{Semi-conjugate models generally}

\begin{sblock}{Conjugate models}
Conjugacy can be defined as follows \cite{gelman2013bayesian}. If $\mathcal{F}$ is a class of sampling distributions and $\mathcal{P}$ is a class of prior distributions for $\theta$, then the class $\mathcal{P}$ is \textit{conjugate} for $\mathcal{F}$ if
\[  p(\theta \cond y ) \in \mathcal{P} \; \text{for all} \; p(\cdot \cond \theta) \in \mathcal{F} \; \text{and} \; p(\cdot) \in \mathcal{P} \]
\end{sblock}

\begin{sblock}{Semi-conjugate models}
Conditional conjugacy (sometimes called semi-conjugacy) can be defined similarly  \cite{gelman2013bayesian}.  If $\mathcal{F}$ is a class of sampling distributions and $\mathcal{P}$ is a class of prior distributions for $\theta \cond \phi$, then the class $\mathcal{P}$ is \textit{conditionally conjugate} for $\mathcal{F}$ if
\[  p(\theta \cond \phi, y) \in \mathcal{P} \; \text{for all} \; p(\cdot \cond \theta, \phi) \in \mathcal{F} \; \text{and} \; p(\cdot \cond \phi) \in \mathcal{P} \]
\end{sblock}
\pause 
\bottomtext{In other words,  a family of prior distributions for a parameter is called conditionally conjugate if the conditional posterior distribution (often called the \textit{complete conditional}),  given the data and all other parameters in the model,  is also in that class.}
\end{frame}

\begin{frame}{We {\red \heart} semi-conjugacy}	 
 Why are conditionally conjugate models of interest?  The posterior distributions for conditionally conjugate models are easily approximated with Gibbs sampling or Mean Field Variational Inference -- the former samples from the complete conditional,  whereas the latter takes variational expectations with respect to the natural parameter of the complete conditional.   
 
\end{frame}


	
\begin{frame}{Complete conditionals for the Bayesian MVN}
	
We sample from the posterior by iteratively sampling from the \textit{complete conditionals}:
\footnotesize 
\begin{align*}
\+\mu  \cond \+\Sigma, \+x &\sim \N_{d}(\+m,\+V ) \\ %\labelit\label{eqn:normal_model_complete_conditional_on_mu} \\
\intertext{where}
\+m  &=  \bp{\+V_0^{-1} + N  \+\Sigma^{-1} }^{-1}  \bp{\+V_0^{-1} \+m_0 + N \+\Sigma^{-1}  \bar{\+x} } \\
\+V &= \bp{\+V_0^{-1} +  N \+\Sigma^{-1} }^{-1} \\
\intertext{and}
\+\Sigma \cond \+\mu,  \+x  &\sim \InverseWishart(\nu,  \+\Psi) \\
%\labelit \label{eqn:normal_model_complete_conditional_on_Sigma} \\
\intertext{where}
\nu &=  \nu_0 + N \\
\+\Psi &= \Psi_0 + \ds\sum_{i=1}^N  (\+x_i - \+\mu) (\+x_i - \+\mu)^T 
%\labelit \label{eqn:Sigma_cc_normal_model_cond_conj_prior}\\ 
\end{align*}
\end{frame}


 
\begin{frame}{Complete conditionals: Interpretation}
These complete conditionals have nice interpretations:
\begin{itemize}
\item \textbf{Complete conditional for $(\+\mu  \cond \+\Sigma, \+x)$}: On the precision scale,  $\+V$ is the sum of the prior precision matrix $\+V_0^{-1}$ and $N$ copies of the precision for each observation,  $\+\Sigma^{-1}$.    Similarly,  $\+m$ is the precision-weighted convex combination of $\+m_0$, the prior mean,    and the empirical average, $\bar{\+x}$.
\item \textbf{Complete conditional for $(\+\Sigma \cond \+\mu,  \+x)$}:  The covariance was estimated from $\nu$ observations with a sum of pairwise deviation products $\Psi$.
\end{itemize}
\end{frame}

\begin{frame}{Complete conditionals: Proof}

See exponential family notes. 

\end{frame}
	
	

%%%%%
\section{Application: Reading Comprehension}

\begin{frame}

See ipython notebook. 	
\end{frame}


%%%%%
\section{Missing data and imputation}

\begin{frame}{References}

\begin{center}
\includegraphics[width=.5\textwidth]{images/hoff_book}
\end{center}

\end{frame}

\begin{frame}{Pima Dataset}

\begin{figure}
\includegraphics[width=.6\textwidth]{images/pima_scatterplots}
\caption{  \footnotesize Univariate histograms and bivariate scatterplots for four variables taken from a dataset involving health-related measurements on 200 women of Pima Indian heritage living near Phoenix,  Arizona.  \scriptsize The four variables are \texttt{glu} (blood plasma glucose concentration),  \texttt{bp} (diastolic blood pressure),  \texttt{skin} (skin fold thickness),  and  \texttt{bmi} (body mass index).  }
\end{figure}

\end{frame}

\begin{frame}{Pima Dataset}

\begin{figure}
\includegraphics[width=.6\textwidth]{images/pima_first_ten_entries}
\caption{  \footnotesize Entries for the first ten subjects in the dataset.  \scriptsize The \texttt{NA}'s stand for ``not available." }
\end{figure}
\end{frame}

\begin{frame}{Description of problem}
How to do parameter estimation in the presence of missing data? \\
\begin{itemize}
\item[]  \footnotesize  We cannot do parameter estimation,  because we cannot compute the likelihood $\prod_{i=1}^n p(\+y_i \cond \+\theta)$.  \normalsize
\end{itemize}
\vfill \vfill 
Two common approaches taken by software packages:
\begin{enumerate}
\item Throw away all subjects with missing data
\pause  \\
\quad \redx \footnotesize Discards a potentially large amount of useful information.  \normalsize
\pause \\
\item  Impute the population mean or some other fixed value. 
\pause \\
\quad \redx \footnotesize  Assumes certainty about these values,  when in fact we have not observed them.  \normalsize
% DELIVERY: Elicit audience suggestions as to the problems with each common approach.
\end{enumerate}
\end{frame}


\begin{frame}{Missing at random (MAR)}
Let $\+O_i = (O_1, ..., O_p)^T$ be a binary vector such that
\begin{itemize}
\item $O_{ij} = 1 \implies  Y_{ij}  \; \text{is observed}$ 
\item $O_{ij} = 0 \implies  Y_{ij}  \; \text{is missing}$ 
\end{itemize}

\vfill

\begin{definition}
We say the missing data are \textit{missing at random} if $\+O_i$ and $\+Y_i$ are conditionally independent given the model parameters  $\+\theta$  and the distribution of $\+O_i$ does not depend on $\+\theta$.
% TODO: Check this definition;  I altered Hoff's to be more clear based on what seemed to be used in the upcoming formula.
\end{definition}


\vfill  \vfill \vfill  \pause 

\scriptsize
\textbf{Remark.}  This is one of the three types of missingness.   In gist:
\begin{itemize}
\item Missing completely at random (MCAR) - missingness is independent of all data
\item Missing at random (MAR) - missingness is independent of observed data
\item Missing not at random (MNAR) - missingness depends on missing values (and perhaps observed data)
\end{itemize}
% QUESTION: Does this characterization (grabbed from some slides) comport with the one I gave for MAR above? No mention of parameters here.   Perhaps the frequentist characterization differs form the Bayesian one?


\end{frame}


\begin{frame}{The likelihood in the presence of MAR data}

When the data is missing at random,  the sampling probability (density) for the data from observational unit $i$ is given by

\begin{align*}
p(\+o_i,  \set{y_{ij} : o_{ij} =1} \cond \+\theta) & \stackrel{(1)}{=} p(\+o_i) \;  p(\set{y_{ij} : o_{ij} =1} \cond \+\theta) \\
&=  p(\+o_i) \;  \ds\int p(y_{i1}, ...., y_{ip}  \cond \+\theta)  \prod_{y_{ij} : o_{ij} = 0} dy_{ij}
\end{align*}
\footnotesize where in (1) we applied the definition of MAR. \normalsize

\vfill \vfill \pause

\greencheck So in the presence of MAR data, the correct thing to do is \textit{integrate over} the missing data to obtain the marginal probability (density) of the observed data. 
\end{frame}


\begin{frame}{Utilization in multivariate normal models}


In the case of multivariate normal models \footnotesize (so $\+\theta = (\+\mu, \+\Sigma)$) \normalsize, the integration is easy:  Multivariate normals have normal marginals. 

\vfill \vfill \vfill 
\begin{example}
Suppose $\+y_i = (y_{i1},  \texttt{NA},  y_{i3},  \texttt{NA})^T$, so $\+o_i = (1,0,1,0)^T$.     

Then

\begin{align*}
p(\+o_i,  y_{i1},  y_{i3} \cond \+\mu, \+\Sigma) & = p(\+o_i) \;  p(y_{i1},  y_{i3} \cond \+\mu, \+\Sigma) \\
&=  p(\+o_i) \;  \ds\int p( \+y_i  \cond \+\mu, \+\Sigma)  \; dy_2 \; dy_4
\end{align*}
\vfill 

The marginal density $p(y_{i1},  y_{i3} \cond \+\theta)$ is simply a bivariate normal density with mean $(\mu_1,  \mu_3)^T$ and covariance matrix made up of $(\sigma^2_1,  \sigma_{13},  \sigma^2_3)$.
\end{example}

\end{frame}

\begin{frame}{Gibbs sampling with missing data}

\begin{sblock}{Complete data}
If $\+Y$ is the $n \times p$ matrix in which $o_{i,j}=1$ if $Y_{i,j}$ is observed and $o_{i,j} = 0$ if $Y_{i,j}$ is missing, then $\+Y$ has two parts

\begin{itemize}
\item $\+Y_{\text{obs}} := \{y_{i,j} : o_{i,j} =1 \}$, the data that we observe, and 
\item 	$\+Y_{\text{miss}} := \{y_{i,j} : o_{i,j} =0 \}$, the data that we do not observe.
\end{itemize}
\end{sblock}

\begin{sblock}{Gibbs sampler}
A Gibbs sampling scheme for approximating the posterior is given by:

\begin{enumerate}
\item Sampling $\+\mu^{(s+1)}$ from $p(\+\mu \cond \+Y_{\text{obs}}, \+Y_{\text{miss}}^{(s)}, \+\Sigma^{(s)})$;
\item Sampling $\+\Sigma^{(s+1)}$ from $p(\+\Sigma \cond \+Y_\text{obs}, \+Y_\text{miss}^{(s)}, \+\mu^{(s+1)})$;
\item Sampling $\+Y_\text{miss}^{(s+1)}$ from $p(\+Y_\text{miss} \cond \+Y_\text{obs}, \+\mu^{(s+1)}, \+\Sigma^{(s+1)})$;
\end{enumerate}
\end{sblock}

\pause
\bottomtext{The first two steps are the same as before!  The third step is covered in the next slide.  Any guesses?}

\end{frame}


\begin{frame}{Sampling the missing data}

\scriptsize 
\begin{align*}
p(\+Y_\text{miss} \cond \+Y_\text{obs}, \+\mu, \+\Sigma) & \propto p(\+Y_\text{miss},  \+Y_\text{obs} \cond  \+\mu, \+\Sigma) \\
&= \ds\prod_{i=1}^n p(\+y_{i, \text{miss}}, \+y_{i, \text{obs}} \cond \+\mu, \+\Sigma) \\
&\propto \ds\prod_{i=1}^n p(\+y_{i, \text{miss}} \cond \+y_{i, \text{obs}},  \+\mu, \+\Sigma) \\
\end{align*}

How to proceed? \pause  We apply standard results about conditional distributions formed from partitions of multivariate normals:

\begin{align*}
\+y_{[b]} \cond \+y_{[a]}, \+\mu, \+\Sigma & \sim \mathcal{MVN} \bigg( \+\mu_{b|a}, \+\Sigma_{b|a}  \bigg), \quad \text{where} \\
\+\mu_{b|a} &= \+\mu_{[b]} + \+\Sigma_{[b,a]} (\+\Sigma_{[a,a]})^{-1} (\+y_{[a]} - \+\mu_{[a]}) \\
\+\Sigma_{b|a}  &= \+\Sigma_{[b,b]} -  \+\Sigma_{[b,a]} (\+\Sigma_{[a,a]})^{-1} 	\Sigma_{[a,b]}
\end{align*}

\bottomtext{Some macroscopic properties:  \pause 
	\begin{itemize}
	\item The conditional mean, $\+\mu_{b|a}$, starts off at the unconditional mean, $\+\mu_{[b]}$, but then is modified by $(\+y_{[a]} - \+\mu_{[a]})$ in a way that depends on the covariance $\+\Sigma_{[b,a]}$.
	\item The conditional variance $\+\Sigma_{b|a}$ is less than the unconditional variance $\+\Sigma_{[b,b]}$  
	\end{itemize}
}

\end{frame}

\begin{frame}{Posterior Correlations}

\begin{columns}
\begin{column}{0.6\textwidth}

To each covariance matrix there corresponds a correlation matrix $\+C$ given by
\[ \+C := \bigg\{c_{jk} : c_{jk} = \Sigma_{[j,k]} / \sqrt{\Sigma_{[j,j]} \Sigma_{[k,k]}} \bigg\} \]

Simply taking the mean across samples, we obtain the approximation
\[ \E[\+C \cond \+y_1, ..., \+y_n] = 
\begin{bmatrix}
1.00 & 0.23 & 0.25& 0.19 \\
0.23& 1.00& 0.25& 0.24 \\	
0.25& 0.25& 1.00& 0.65 \\	
0.19 &0.24&0.65&1.00 \\		
\end{bmatrix}
\]

\scriptsize 
Notes:
\begin{itemize}
\item Bayesian paradigm again yielding unlimited access to posterior functionals of interest,  without doing any extra inferential work!   % Compare to frequentist or variational inference 
\item Correlations are generally of interest for multivariate normal models, but they are \textit{especially} relevant to imputation. % used to improve how misssing values are filled in.
\end{itemize}
\end{column}

\begin{column}{0.4\textwidth}
\begin{figure}
\includegraphics[width=.7\textwidth]{images/pima_correlations}
\caption{95\% posterior confidence intervals for correlations}
\end{figure}
\end{column}


\end{columns}
 \end{frame}

\begin{frame}{Intelligent imputations}
The posterior expectation gives a much better imputation than some flat fixed value.

\begin{figure}
\includegraphics[width=.45\textwidth]{images/pima_missing_vs_true}
\caption{True values of the missing data vs. posterior expectations}
\end{figure}

\pause 
 Imputations are especially good for \texttt{skin} and \texttt{bmi},  due to their higher correlations.  

\end{frame}



\end{document}



