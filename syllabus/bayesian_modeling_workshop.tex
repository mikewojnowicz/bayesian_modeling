
%\pdfoutput=1
%\synctex=1

\documentclass{article} % For LaTeX2e

%STANDARD PREAMBLE
%https://tex.stackexchange.com/questions/68821/is-it-possible-to-create-a-latex-preamble-header
\usepackage{/Users/mwojno01/Research/Learning/latex_preamble/preamble}

\begin{document}

\title{Introduction to Bayesian Modeling} 

\maketitle

\tableofcontents
\newpage

\section{Overview}

\subsection{Goal}  The goal of this workshop is to introduce students to the concepts and practice of Bayesian modeling.   We will begin by motivating Bayesian approaches.  Next,  we will introduce and apply models with conjugate priors,  such as Bayesian normal,  Bayesian binomial,  and Bayesian linear regression.   We will then introduce the two primary techniques for approximate Bayesian inference,  namely Markov Chain Monte Carlo (MCMC) and variational inference.    Using these techniques,  and in some cases clever trickery,  we will then tackle models for which there are not conjugate priors,  such as Bayesian logistic regression,  Bayesian multiclass regression,  Bayesian mixture models,  and Bayesian hidden Markov models.  Finally,  we will very briefly discuss Bayesian deep learning.    For applications,  we will use Python; namely,  a combination of  pymc3,  scikit-learn,  and code we write ourselves.  
 
\subsection{Date and Time}  The workshop will be held via Zoom,  June 7-11,  from 2pm-5pm EST.   
 
\subsection{Target Audience}  We expect that the typical student will be a graduate student, faculty member, staff member, or researcher in a quantitative field (such as computer science,  statistics,  engineering, or biology),  who would like to learn more about Bayesian modeling.  

\subsection{Prerequisites} Prerequisites include calculus,  some familiarity with introductory linear algebra (matrix multiplications,  determinants,  and traces),  and some familiarity with introductory probability (e.g.,  we will assume prior fluency with concepts such as expectation,  conditional probability,  and commonly used distributions,  such as Gaussian and Poisson).    The class will use Python as a common language.     The workshop will employ student-centered components;  for maximum benefits, we strongly encourage setting aside 1-2 hours per day outside of the workshop to work on material.    %The workshop will employ student-centered components so \textbf{be prepared to spend 3-4 hours a day outside of the workshop working on material.}  This time will be spent on  background reading and preparing code/demos/materials for the interactive component of the workshop.

\subsection{Textbook}

A textbook is not needed for this workshop.  However,  three excellent resources are:

\begin{enumerate}
\item Peter Hoff's textbook \cite{hoff2009first}
\item Christopher Bishop's textbook \cite{bishop2006pattern}
\item Andrew Gelman et al.'s textbook \cite{gelman2013bayesian},  available online at \url{http://www.stat.columbia.edu/~gelman/book/}. 
\end{enumerate}

We will draw especially heavily from \cite{hoff2009first}.

\subsection{Structure}

\subsubsection{Philosophy}

\textit{Learning in order to create} is both more fun and more effective than \textit{learning for some extrinsic purpose}.   Hence,  the workshop is structured so as to (a) be student-centered and (b) allow self-determination and autonomy in how students engage with the material. 

\subsubsection{Format}

Between 1/2-2/3 of the workshop will involve lectures via slides.   

Between 1/3-1/2 of the workshop will be interactive,  including:

\begin{itemize}
\item Interactive exercises:
	\begin{itemize}
	\item Google Collab exercises using iPython notebook
	\item Constructing and/or working with Python (rather than R) implementations of \cite{hoff2009first} and \cite{gelman2013bayesian}.
	\end{itemize}
\item Student ``lightning chat" (5 minute) presentations + discussions.    Schedule in 2-3 per day.   To allow for student-centered direction and autonomy,  students may choose any of the following:
	\begin{itemize}
	\item Presentation of Python implementations of models from \cite{hoff2009first} ,  \cite{ gelman2013bayesian},  or the workshop.
	\item Presentation of an exercise from \cite{gelman2013bayesian} or \cite{hoff2009first}.
	\item Reviewing a demo with us from \url{https://github.com/avehtari/BDA_py_demos}.
	\item Presentation of a reading section,  blog,  etc.  of interest.
	\item Presentation of a mathematical derivation of something relevant to the course.
	\item Presentation of how a concept relates to something from their research area.
	%\item A list of questions that we can discuss.
	\end{itemize}
\item (Maybe) Mini reading group discussions  -- We likely won't have time to read a whole paper,  but we could discuss sections of a text at the very beginning, or perhaps sections of a relevant paper.
\end{itemize}


\section{Topics}

Below are topics we plan to cover in the course:

\subsection{Introduction to Bayes}

We present everything in here using conjugate models with closed-form posteriors.  The models are useful in and of themselves,  as well as to build intuition for more complicated models.   

\begin{itemize}
\item \textbf{Why Bayes?} -- See Section 1.2 of \cite{hoff2009first}.   \cite{bishop2006pattern} has some nice plots motivating why use Bayesian linear regression over standard linear regression.   \cite{ghahramani2013bayesian}  has some nice plots illustrating the Bayesian approach and how it mitigates overfitting.   I can provide a nice example with biometric profiling of human typing dynamics.   \cite{held2006bayesian} has a nice simple example of obtaining non-standard functionals from the posterior that can be of interest.   \cite{wilson2020case} presents the case for Bayesian deep learning.  
\item \textbf{Introduction to inference} 
	\begin{itemize}
	\item \textbf{Belief functions,  Bayes rule} -- Sections 2.1,  2.2 of \cite{hoff2009first}.   \cite{ghahramani2013bayesian} briefly overviews the Bayesian framework.   For important mistakes in real life in medicine and law,  see \cite{guardianXXXXobscure}.  \textit{Why most published research findings are false} \cite{ioannidis2005most} provides nice additional motivation in science.    Could perhaps cover exchangeability here. 
	\item \textbf{Single-parameter conjugate models} - Chapter 11. 2 of \cite{davison2003statistical} has a nice very brief introduction to inference.      Sections 3.1 and 3.2 of \cite{hoff2009first} cover the  binomial and Poisson models.   Introduce the exponential family formalism (see \cite{wojnowiczXXXXexponential};  see also Section 5.2 of \cite{davison2003statistical} ) for much greater breadth.  
	\item \textbf{Bayesian multivariate normal} -  Section 7 of \cite{hoff2009first} covers the  multivariate normal model.  The Bayesian MVN is a fundamental model in and of itself.  It also provides some nice opportunities: introducing the notion of semiconjugacy,  discussing imputation for missing data, and motivating Gibbs sampling.   Note that the Bayesian MVN can be either conjugate or semi-conjugate,  depending on the choice of prior.
	\end{itemize}   

\end{itemize}

%We will want to find a way to get students to group up,  probably based on domain expertise/interests,  so that they can eventually work together on a project.  



\subsection{Semi-conjugate models} \label{sec:semi_conjugacy}

To do inference with semi-conjugate models, we need to introduce MCMC and VI.  These are the two primary methods for doing inference on Bayesian models without closed-form posteriors. In Section \ref{sec:non_conjugacy}, we will push these methods further to the non-conjugate case.

\subsubsection{Methods}

\begin{itemize}
\item \textbf{Intro to MCMC sampling} - Basic sampling methods - inverse cdf,  rejection sampling. Basic Markov Chain Monte Carlo (MCMC) - Metropolis Hastings, Gibbs sampling.
\item \textbf{Introduction to variational inference} - Coordinate ascent variational inference (CAVI) \cite{wojnowiczXXXXfoundations}.
\end{itemize} 

\subsubsection{Models}
These models are often semi-conjugate, in which case inference prototypically proceeds via Gibbs sampling or CAVI. 

\begin{itemize}
\item \textbf{Mixture models} - Brief presentation of Gaussian mixture models; will occur alongside CAVI.
\item \textbf{Hidden markov models} -   We will probably need to skip this -- or make it super fast -- in the interest of time.   But might give brief presentation of hidden markov models\footnote{Nice reference for frequentist HMM's: \url{http://jwmi.github.io/ASM/5-HMMs.pdf}}. May give short overview to probabilistic graphical models here.
\item \textbf{Bayesian linear regression} -- Section 9 of \cite{hoff2009first}.     I have notes on this.   There are some nice slides here which also illustrate the use of kernels.\footnote{Nice Bayesian linear regression slides: \url{https://www.cs.toronto.edu/~rgrosse/courses/csc411_f18/slides/lec19-slides.pdf}}   Introduce model selection here as well (Section 9.3 of \cite{hoff2009first} or Section 3.4 of \cite{bishop2006pattern}).   Section 11.2.2 of \cite{davison2003statistical} also has a nice,  quick summary of Bayes factors for model comparison.   
\item \textbf{Hierarchical models} - Section 11.4 of \cite{davison2003statistical} has a nice brief overview.    Hierarchical normal model (e.g. Gelman's 5 schools example).   Hierarchical linear regression (Chapter 13 of \cite{gelman2013bayesian},   Secs 11.1-11.3 of \cite{hoff2009first}.).  Figure 11.1 and 11.3 (right) of \cite{hoff2009first} nicely shows the beneficial effect of sharing statistical strength in a hierarchical linear regression,  as compared to many separate linear regressions.  
\end{itemize}

\subsection{Non-conjugate models} \label{sec:non_conjugacy}

Here, again, we use MCMC and VI, but we need to employ more advanced (or at least more general) techniques.

\subsubsection{Methods}

\begin{itemize}
	\item \textbf{Gradient-based sampling} - A brief introduction to Hamiltonian Monte Carlo (HMC) and No U-Turn Sampler (NUTS).  Introduction to the python package \texttt{pymc3}.
	\item \textbf{Model augmentation} - An auxilliary variable may be introduced to transform a non-conjugate model into one that is conjugate or semi-conjugate.
	\item \textbf{Variational inference for non-conjugate models} - We will probably need to skip this in the interest of time.  Common strategies are Laplace method VI,  Delta method VI, and automatic differentiation VI.
\end{itemize}
\subsubsection{Models}

\begin{itemize}
\item \textbf{Categorical models} - Includes logistic regression, probit regression,  binomial,  multinomial,  etc.  Cover the auxilliary variable trick, if there is time. See also pp.  390 of \cite{hoff2009first} for a useful warm-starting strategy.  
\item \textbf{Bayesian deep learning models} -  Bayes and neural networks.   20-30 min w/ guest presenter,  Kyle Heuton,  Ph.D.  student,  computer science.
\end{itemize}

%\subsection{Even more complicated models} 
%
%Here we discuss models which have additional complexity -- they could involve neural networks,  non-parametrics,  larger scale,  etc.    Often these involve some additional inferential machinery -- \textit{stochastic} variational inference,  reparametrization trick,  etc.
%
%%The likelihood that students would use this material for their projects is relatively low,  so this would make a nice set of things to cover as we begin transitioning over to the student project day.
%
%\begin{itemize}
%\item \textbf{Why Bayesian Deep Learning?}  Bayes and neural networks.   20-30 min w/ guest presenter,  Kyle Heuton,  Ph.D.  student,  computer science.
%\item \textbf{Custom models}  Could present black-box variational inference or automatic differentiation variational inference here.   I have some notes that I could convert to slides.  Would take some work though.  \blue{Anna,  perhaps you'd be interested,  since you've been working with this?}
%\item \textbf{Sampler platter of other possible topics}   VAE,   Gaussian processes,  composing time series models with neural networks,  indian buffet processes,  dirichlet process mixture models,  etc.  
%\end{itemize}

%\subsection{Bayesian workflow}  \label{sec:Bayesian workflow}
%
%
%Lots of nice resources for Bayesian workflow.    For example: \cite{gelman2020bayesian} or \cite{gabry2019visualization}.   Section 6 of \cite{gelman2013bayesian} covers model checking.       
%
%We could cover this the day before the student projects -- which should help them as they wrap up their projects -- and leave lots of time/space for workshopping. 
%
%Some points to make re: model checking
%
%\begin{itemize}
%\item \textit{Samples from the posterior predictive should capture important properties of the observed dataset.}  For a violation of this,  see the normal model for Newcomb's speed of light measurements.  (Compare Figures 6.2 and 3.1 of \cite{gelman2013bayesian}.)
%\end{itemize}
%
%\subsection{Student project presentations}
%
%For student projects,  we could have students present results of a Bayesian data analysis mini-project.   Perhaps they could highlight some aspect of the Bayesian workflow along the way. 
%
%We would probably want to have a ``workshopping" section the day before they do their presentations.


\bibliography{references_bayesian_modeling_workshop}{}
\bibliographystyle{unsrt}


\appendix

\section{Student mini-presentations}

\subsection{Intro} 
\begin{itemize}
\item Present \textit{Why most published research findings are false. } \cite{ioannidis2005most}.
\item Present Bayesian analysis of multiple sudden infant deaths.  \cite{hill2004multiple}.
\item Textbook sections TBD. 
\end{itemize}

\subsection{Model Checking} 
\begin{itemize}
\item Textbook sections TBD. 
\end{itemize}

\section{Mini projects}

These are small problems to work on.  Some are open ended.

\subsection{Missing data}

This project covers missing data and imputation.  Imputation is made part of inference,  rather than a separate step. 

\begin{itemize}
\item  Inference with a Bayesian normal model in the presence of missing data is given,  in R code,  on pp. 119 of \cite{hoff2009first}.    Implement,  and test,  the code in Python.
\end{itemize}

\subsection{Batting average dataset}

The hierarchical normal model for (arcsine-transformed) batting average data on pp.  163 of \cite{gelman2013bayesian} has some serious deficiencies,  as exposed in Table 6.1 in the section on model checking.    

Can you construct (and learn) a better model which makes predictions closer to the true final batting average?
 
Examples:
\begin{itemize}
\item Add an extra layer to the hierarchy,  so that player $p$'s 1970 batting average inherits from player $p$'s overall batting average which in turn inherits from a population batting average.  (Of course,  I am speaking of the arcsine-transformed batting averages,  so that we can use a hierarchical normal model.)
\item Add an autoregressive component, because,  as mentioned by Gelman,  player batting averages \textit{DO} change over time. 
\end{itemize}

The text also does a poor job of checking the modeling assumption violations that were of concern.   Can you do a better job of checking them,  and if necessary,  address them?

Examples:
\begin{itemize}
\item If batting averages are indeed heavy tailed or skewed,  move from a normal distribution to something else.   For example,  could try a t-distribution with Laplace inference to handle the non-conjugacy. 
\item If the variance is indeed too high for a binomial model,  try something that can handle the overdispersion. 
\end{itemize}

\section{Some topics we won't get to}

\begin{itemize}
\item \textbf{Bayesian workflow} -- Lots of nice resources for Bayesian workflow.    For example: \cite{gelman2020bayesian} or \cite{gabry2019visualization}.   Section 6 of \cite{gelman2013bayesian} covers model checking,  as does  Section 11.2.3 of \cite{davison2003statistical}.    Some points to make re: model checking
	\begin{itemize}
	\item \textit{Samples from the posterior predictive should capture important properties of the observed dataset.}  For a violation of this,  see the normal model for Newcomb's speed of light measurements.  (Compare Figures 6.2 and 3.1 of \cite{gelman2013bayesian}.)
	\end{itemize}
\item \textbf{Bayesian GLM's and GLMM's} - Bayesian GLM's would extend the section on ``Regression models for binary and multi-class data" to other distributions, and present models in greater generality.  Bayesian GLMM's are the hierarchical extension.  Some inference techniques that could be useful here include:  auxilliary variable trick, HMC, Laplace variational inference. Note that intelligent handling of semi-conjugacy can still be useful.
\item \textbf{Bayesian time series models} -  We just covered hidden markov models\footnote{Nice reference for frequentist HMM's: \url{http://jwmi.github.io/ASM/5-HMMs.pdf}}, but it would be nice to also introduce state space models.    Could mention embedding of GLM's or GLMM's within them.   Could give some overview to probabilistic graphical models here. 
\item \textbf{VI for nonconjugate models} - For instance, we could cover Laplace and Delta Method VI as applied to categorical models.   We could also cover black box / automatic differentiation VI.
\end{itemize}

\section{Notes to self:  some details for when slides are constructed}

\subsection{Why Bayes}

\paragraph{Bayes law in real life}
The primary motivation here is the importance of not ``flipping the conditional" during interpretation.   Sally Clark was convicted of murdering her two children,  because the chance of two babies dying of SIDS in one family was one in 73m.   But the expert witness ignored the prior probability that someone was a double murderer \cite{guardianXXXXobscure}.  (See \cite{hill2004multiple} for discussion.)      Could also refer to the classic example with positive tests - maybe in reference to COVID \cite{guardianXXXXobscure}.     \textit{Why most published research findings are false} \cite{ioannidis2005most} provides a third example. 

\paragraph{Modeling Application: Estimating the probability of a rare event}
See Section 1.2.1 of \cite{hoff2009first}.  The problem is to estimate the proportion of people in a small town that have a disease given a small sample of 20 individuals.     This is a nice believable example in which prior information is natural: we use prevalence in similar towns.  (Make a note that really this is foreshadowing hierarchical models and hierarchical regressions!)  This also illustrates another nice property of Bayes  -- we can get lots of functionals from the posterior.  The plot on the right of Figure 1.1 -- showing the shift between the prior and posterior distributon -- is a classic.   The sensitivity analysis is pretty cool -- see the right hand plot of Figure 1.2,  as well as the last couple sentences on pp.7.    

The REAL kicker,  I think,  is the comparison to non-Bayesian methods.  The frequentist confidence interval is complete garbage here,  and the ``adjusted" Wald interval is clearly just a (very specific) choice of .prior.   Nice opportunity for discussion here.  Ask:  what's the advantage of Bayes over that?  Possible answers: That is seemingly ad hoc; it's not flexible to other choice of priors; Bayes makes it clear how it relates to priors; and it doesn't allow for sensitivity analysis (or investigation of various functionals).  



\subsection{Introduction to inference}

\paragraph{Bayes factors}

Very nice brief discussion in Section 11.2.2 of \cite{davison2003statistical}. 

\paragraph{Multivariate normal - missing data and imputation}

See Section 7.5 of \cite{hoff2009first}.

\subsection{Hierarchical models}

Some motivations:  
\begin{itemize}
\item A way to use ``surrounding data" as a prior in a more formal way.    Think back to Hoff's disease prevalence example.    In that case,  we constructed our beta prior manually,  by taking a couple of basic facts about similar towns and then converting that into beta parameters.    A hierarchical model could let the prior expectation be tied more exactly to those surrounding towns (including, if a regression is involved,  similarity w.r.t. \text{relevant} characteristics,  such as size,  SES,  etc.),  to automatically set the strength of the prior expectation according to the relative uncertainty within and between towns,  and to automatically adapt as data rolls in. 
\item Can be convenient.  Consider Wand's construction of the half-t distribution as a hierarchical model of inverse gammas,  which makes for a conditionally conjugate scheme.  Basically any auxilliary variable trick (think Polya Gamma augmentation) can be considered as the construction of a hierarchical model for computational convenience.  
\item By adding layers to the hierarchy,  we can escape bad assumptions -- e.g.,  the beta-binomial model handles over-dispersion.   (This might be a better example for "why Bayes".)
\end{itemize}


%\section{Need to do}
%
%\begin{itemize}
%\item \blue{Karin}  Bring pymc3,  stan,  etc.  into this -- will make things a lot more useful for the audience than requiring that they code things up from scratch!
%\end{itemize}

%\subsection{Regression models for binary and multi-class data}
%
%Take the batting average problem on pp.  163 of \cite{gelman2013bayesian} and use a more conventional modeling approach (logit or probit regression) than what was described in the text (using the arcsine transformation).   What kinds of inference results do you get?  How far off is  Polya-Gamma variational inference?  What about Laplace variational inference? 


          


\end{document}
